# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yPELy4zOplHzwf1Ul372Bh-kyeK-fgwg
"""
## Dependencies
#pip install langchain chromadb pypdf langchain_google_genai

import os
import csv
import torch

import google.generativeai as genai

from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain.document_loaders import PyPDFLoader
from langchain.prompts import PromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains.summarize import load_summarize_chain

# Model Initailization
def load_model(gem=True):

    if gem:
      llm = ChatGoogleGenerativeAI(model="gemini-pro",google_api_key=GOOGLE_API_KEY,
                             temperature=0.2,convert_system_message_to_human=True)

    else:
      """
      Initialize and return the tokenizer and base model for question or answer generation.

      Parameters:
      - ques (bool): If True, initializes for question generation; otherwise, initializes for answer generation.

      Returns:
      - tokenizer: Hugging Face tokenizer
      - base_model: Hugging Face base model
      """
      checkpoint = "MBZUAI/LaMini-Flan-T5-783M"
      tokenizer = AutoTokenizer.from_pretrained(checkpoint)
      base_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

      """
      Load the Language Model (LLM) into a transformer pipeline for question or answer generation.

      Parameters:
      - ques (bool): If True, loads the model for question generation; otherwise, loads the model for answer generation.

      Returns:
      - llm: HuggingFacePipeline for question or answer generation
      """
      pipe = pipeline(
      'text2text-generation',
      model = base_model,
      tokenizer = tokenizer,
      device = 0 if torch.cuda.is_available() else -1,
      max_length = 512,
      do_sample = True,
      temperature = 0.8,
      top_p= 0.85
      )
      llm = HuggingFacePipeline(pipeline=pipe)

    return llm

def load_embeddings(gem=True):
    #Latent Space
    if gem:
      embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001",google_api_key=GOOGLE_API_KEY)
    else:
      embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return embeddings

def file_processing(file_path):
    """
    Process a PDF file, extracting content and splitting it into chunks for further processing.

    Parameters:
    - file_path (str): Path to the PDF file.

    Returns:
    - document_ques_gen: List of Document objects for question generation
    - document_answer_gen: List of Document objects for answer generation
    """

    # Load data from PDF
    loader = PyPDFLoader(file_path)
    data = loader.load()

    question_gen = ''

    for page in data:
        question_gen += page.page_content

    splitter_ques_gen = RecursiveCharacterTextSplitter(
        chunk_size = 1000,
        chunk_overlap = 250
    )
    # Chunking
    chunks_ques_gen = splitter_ques_gen.split_text(question_gen)

    document_ques_gen = [Document(page_content=t) for t in chunks_ques_gen]

    splitter_ans_gen = RecursiveCharacterTextSplitter(
        chunk_size = 300,
        chunk_overlap = 75
    )

    # Chunking
    document_answer_gen = splitter_ans_gen.split_documents(
        document_ques_gen
    )

    return document_ques_gen, document_answer_gen

def llm_pipeline(file_path, gem=True):
    """
    Run the Language Model (LLM) pipeline for question and answer generation.

    Parameters:
    - file_path (str): Path to the PDF file.

    Returns:
    - answer_generation_chain: RetrievalQA chain for answer generation
    - filtered_ques_list: List of filtered questions
    """

    document_ques_gen, document_answer_gen = file_processing(file_path)
    # Model
    llm_ques_gen_pipeline = load_model()

    prompt_template = """
    I am preparing a researcher for their upcoming tests by creating insightful and diverse questions based on their study material.
    The goal is to cover all critical aspects of the provided text to ensure comprehensive preparation.
    Below is the text from which I need to generate a wide range of questions, aiming to explore every important detail mentioned.
    Please help me by formulating as many relevant questions as possible, with a minimum goal of five, to thoroughly prepare the researcher based on this text:

    -------------
    {text}
    -------------

    Please focus on generating questions that encompass all key information, concepts, and nuances present in the text. These questions should be diverse in nature, including but not limited to, understanding basic facts, interpreting data, exploring implications, and applying the information in hypothetical scenarios.

    QUESTIONS:
    """

    PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=["text"])

    refine_template = ("""
    Given the text provided, you are tasked with creating insightful practice questions to aid a researcher in their preparation.
    Your expertise in crafting questions from textual material, including PDFs and documents, is essential here.
    Based on the content of the text, generate a diverse set of at least 5 questions, expanding to as many as you find relevant.
    You may refine the existing questions provided or introduce new ones to enhance the researcher's study experience. Use the additional context provided, if necessary, to refine or create questions that are insightful and relevant to the researcher's needs.
    Existing Questions to Refine or Expand Upon: {existing_answer}
    Text for Question Generation:

    ------------
    {text}
    ------------

    Please refine the existing questions based on the new context or, if you find the context does not add value, suggest original questions derived from the text.

    Refined or New Questions:
    """
    )

    REFINE_PROMPT_QUESTIONS = PromptTemplate(
        input_variables=["existing_answer", "text"],
        template=refine_template,
    )
    # Chain_summarize module
    ques_gen_chain = load_summarize_chain(llm = llm_ques_gen_pipeline,
                                            chain_type = "refine",
                                            verbose = True,
                                            question_prompt=PROMPT_QUESTIONS,
                                            refine_prompt=REFINE_PROMPT_QUESTIONS)

    ques = ques_gen_chain.run(document_ques_gen)
    embeddings = load_embeddings()
    #Vector_DataBase
    vector_store = Chroma.from_documents(document_answer_gen, embeddings)
    # model
    llm_answer_gen = load_model(gem=False)

    ques_list = ques.split("\n")
    # print(ques)
    filtered_ques_list = [element for element in ques_list if element.endswith('?') or element.endswith('.')]
    # #RetrievalAgent_QAchain
    answer_generation_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen,
                                                chain_type="stuff",
                                                retriever=vector_store.as_retriever(search_kwargs={"k":5}),
                                                          )

    return answer_generation_chain, filtered_ques_list

# Saving file to csv
def get_csv (file_path):
    """
    Generate questions, retrieve answers, and save the results to a CSV file.

    Parameters:
    - file_path (str): Path to the PDF file.

    Returns:
    - output_file (str): Path to the generated CSV file.
    """
    answer_generation_chain, ques_list = llm_pipeline(file_path)
    base_folder = 'output/'
    if not os.path.isdir(base_folder):
        os.mkdir(base_folder)
    output_file = base_folder+"QA.csv"
    with open(output_file, "w", newline="", encoding="utf-8") as csvfile:
        csv_writer = csv.writer(csvfile)
        csv_writer.writerow(["Question", "Answer"])  # Writing the header row

        for question in ques_list:
            print("Question: ", question)
            answer = answer_generation_chain.run(question)
            print("Answer: ", answer)
            print("--------------------------------------------------\n\n")

            # Save answer to CSV file
            csv_writer.writerow([question, answer])
    return output_file

get_csv('/content/Attention_is_all_you_need.pdf')

